import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
import pickle
import gzip
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.optimizers import SGD
from sklearn.metrics import roc_curve, auc

# Configuraci√≥n de la p√°gina
st.set_page_config(page_title="An√°lisis Cardiovascular", page_icon="‚ù§Ô∏è", layout="wide")

# Men√∫ de navegaci√≥n en el sidebar
st.sidebar.title("Navegaci√≥n üóÇÔ∏è")
st.sidebar.markdown("---")

# Submen√∫ para An√°lisis de Datos
with st.sidebar.expander("üìä An√°lisis de Datos", expanded=True):
    section_analisis = st.radio(
        "Selecciona una secci√≥n:",
        [
            "Carga de Datos",
            "An√°lisis Exploratorio",
            "Reducci√≥n de Dimensionalidad",
        ],
    )

# Submen√∫ para Modelos de Machine Learning
with st.sidebar.expander("ü§ñ Modelos de Machine Learning", expanded=True):
    section_modelos = st.radio(
        "Selecciona una secci√≥n:",
        [
            "SVM con Datos Originales",
            "SVM con PCA",
            "SVM con t-SNE",
            "Red Neuronal",
        ],
    )

# Submen√∫ para Predicciones
with st.sidebar.expander("üîÆ Predicciones", expanded=True):
    section_predicciones = st.radio(
        "Selecciona una secci√≥n:",
        [
            "Predicci√≥n de Riesgo Cardiovascular",
        ],
    )




# Cargar los datos
@st.cache_data
def load_data():
    try:
        return pd.read_excel('BancoXavantes837.xlsx')
    except Exception as e:
        st.error(f"Error al cargar el archivo: {e}")
        return None

data = load_data()
if data is None:
    st.stop()

# Limpiar nombres de columnas
data.columns = data.columns.str.strip()

# Definir pesos para calcular el √≠ndice de riesgo cardiovascular
pesos = {
    'CTOTAL': 0.2, 'CLDL': 0.3, 'CHDL': -0.2, 'Triglic': 0.2, 'CVLDL': 0.1,
    'IMC': 0.15, 'BAI': 0.1, 'Cintura': 0.15, 'Cadera': -0.1, 'Grasa': 0.1,
    'Edad': 0.2, 'Leptina': 0.05, 'FTO_Aditivo': 0.05
}

# Verificar la existencia de columnas necesarias
missing_columns = [col for col in pesos.keys() if col not in data.columns]
if missing_columns:
    st.error(f"Faltan las siguientes columnas: {missing_columns}")
    st.stop()

# Calcular el √≠ndice de riesgo cardiovascular
data['Riesgo_Cardiovascular'] = data[list(pesos.keys())].mul(pesos).sum(axis=1)

data['Riesgo_Cardiovascular_Binario'] = (data['Riesgo_Cardiovascular'] > data['Riesgo_Cardiovascular'].median()).astype(int)
# Calcular el √≠ndice de riesgo cardiovascular
data['Riesgo_Cardiovascular'] = sum(data[col] * peso for col, peso in pesos.items())

# Calcular el √≠ndice de riesgo cardiovascular
data['Riesgo_Cardiovascular'] = sum(data[col] * peso for col, peso in pesos.items())

# Calcular el √≠ndice de riesgo cardiovascular
data['Riesgo_Cardiovascular'] = sum(data[col] * peso for col, peso in pesos.items())

# Definir un umbral fijo basado en una fracci√≥n del m√°ximo
umbral = data['Riesgo_Cardiovascular'].max() * 0.5  # Ajustar seg√∫n necesidad

# Crear variable binaria de riesgo cardiovascular
data['Riesgo_Cardiovascular_Binario'] = (data['Riesgo_Cardiovascular'] > umbral).astype(int)



# Mostrar datos
st.title('An√°lisis de Enfermedades Cardiovasculares en la Poblaci√≥n Ind√≠gena Xavante de Brasil')
st.write("""
La siguiente base de datos pertenece a una poblaci√≥n de ind√≠genas Xavantes de Brasil, 
la cual cuenta con variables importantes para determinar enfermedades cardiovasculares en la poblaci√≥n.  
La base de datos incluye las siguientes variables:

- **Sexo**: G√©nero de los individuos (hombre o mujer).
- **Edad**: Edad en a√±os.
- **Leptina**: Nivel de leptina, una hormona relacionada con la regulaci√≥n del apetito y el metabolismo.
- **Grasa**: Porcentaje de grasa corporal.
- **IMC**: √çndice de Masa Corporal, una medida de la relaci√≥n entre peso y altura.
- **BAI**: √çndice de Adiposidad Corporal, una medida alternativa al IMC.
- **Cintura**: Circunferencia de la cintura en cent√≠metros.
- **Cadera**: Circunferencia de la cadera en cent√≠metros.
- **CVLDL**: Colesterol de lipoprote√≠nas de muy baja densidad.
- **Triglic**: Nivel de triglic√©ridos en sangre.
- **CTOTAL**: Colesterol total.
- **CLDL**: Colesterol de lipoprote√≠nas de baja densidad (colesterol "malo").
- **CHDL**: Colesterol de lipoprote√≠nas de alta densidad (colesterol "bueno").
- **FTO_Aditivo**: Variante gen√©tica asociada con la obesidad y el riesgo cardiovascular.
""")

st.write("### Vista previa de los datos")
st.dataframe(data.head())
st.write("### Informaci√≥n de los datos")
st.dataframe(data.describe())
# Capturar la informaci√≥n del DataFrame
buffer = io.StringIO()
data.info(buf=buffer)  # Captura la salida de data.info()
info_str = buffer.getvalue()  # Convierte el buffer en string

st.write("### Informaci√≥n del DataFrame")
st.text(info_str)  # Muestra el contenido de info() en formato de texto

st.write("""
### Variable Objetivo: Riesgo Cardiovascular

La variable objetivo de este estudio es el **Riesgo Cardiovascular**, que se determina en funci√≥n de los siguientes criterios cl√≠nicos y umbrales establecidos:

- **Colesterol Total (CTOTAL)**: Alto riesgo si **CTOTAL > 200 mg/dL**.
- **Triglic√©ridos (Triglic)**: Alto riesgo si **Triglic > 150 mg/dL**.
- **Colesterol LDL (CLDL)**: Alto riesgo si **CLDL > 130 mg/dL**.
- **Colesterol HDL (CHDL)**: **Bajo riesgo** si **CHDL < 40 mg/dL (hombres)** o **< 50 mg/dL (mujeres)**.
- **√çndice de Masa Corporal (IMC)**: Alto riesgo si **IMC > 30** (obesidad).
- **Circunferencia de Cintura**: Alto riesgo si **Cintura > 102 cm (hombres)** o **> 88 cm (mujeres)**.
- **Relaci√≥n Cintura-Cadera**: Alto riesgo si **Relaci√≥n > 0.9 (hombres)** o **> 0.85 (mujeres)**.

#### Definici√≥n de la Variable de Inter√©s:
- **0 (Bajo riesgo):** El individuo **no cumple con ninguno** de los criterios de alto riesgo.
- **1 (Alto riesgo):** El individuo **cumple con al menos uno** de los criterios de alto riesgo mencionados anteriormente.

Esta variable se calcula autom√°ticamente en el an√°lisis utilizando los umbrales cl√≠nicos establecidos.
""")

st.write(f"Se ha utilizado un umbral de **{umbral:.2f}** basado en el 50% del valor m√°ximo del √≠ndice.")

# Mostrar el balance de clases
st.write("#### Balance de Clases en la Variable Objetivo")
st.write(data['Riesgo_Cardiovascular_Binario'].value_counts())

st.write("### Histograma de la Distribuci√≥n del Riesgo Cardiovascular")

# Crear el histograma
fig, ax = plt.subplots()
sns.histplot(data['Riesgo_Cardiovascular'], bins=30, kde=True, ax=ax, color='blue')
ax.set_title('Distribuci√≥n del √çndice de Riesgo Cardiovascular')
ax.set_xlabel('Riesgo Cardiovascular')
ax.set_ylabel('Frecuencia')

# Mostrar la gr√°fica en Streamlit
st.pyplot(fig)

st.write("### Distribuci√≥n de Variables Num√©ricas")

# Obtener solo las columnas num√©ricas
numerical_columns = data.select_dtypes(include=['number']).columns.tolist()

# Validar que hay columnas num√©ricas antes de continuar
if not numerical_columns:
    st.error("No se encontraron variables num√©ricas en el dataset.")
    st.stop()

# Seleccionar las variables a visualizar
selected_variables = st.multiselect(
    "Selecciona las variables num√©ricas que deseas visualizar:",
    numerical_columns,
    default=numerical_columns[:2]  # Mostrar las primeras 2 variables por defecto
)

# Verificar si se seleccionaron variables
if not selected_variables:
    st.warning("Por favor, selecciona al menos una variable num√©rica.")
    st.stop()

# Organizar las gr√°ficas en un mosaico
num_columns = 2  # N√∫mero de columnas en el mosaico
num_rows = (len(selected_variables) + num_columns - 1) // num_columns  # Calcular el n√∫mero de filas necesarias

# Crear un mosaico de gr√°ficas
for i in range(num_rows):
    cols = st.columns(num_columns)  # Crear una fila con el n√∫mero de columnas especificado
    for j in range(num_columns):
        idx = i * num_columns + j  # Calcular el √≠ndice de la variable actual
        if idx < len(selected_variables):  # Verificar que el √≠ndice est√© dentro del rango
            variable = selected_variables[idx]
            with cols[j]:  # Usar la columna correspondiente
                fig, ax = plt.subplots(figsize=(6, 4))  # Tama√±o m√°s peque√±o para las gr√°ficas
                sns.histplot(data[variable], bins=30, kde=True, ax=ax)
                ax.set_title(f'Distribuci√≥n de {variable}')
                ax.set_xlabel(variable)
                ax.set_ylabel('Frecuencia')
                st.pyplot(fig)

# Matriz de correlaci√≥n
st.write("#### Matriz de Correlaci√≥n")
fig, ax = plt.subplots(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', ax=ax)
ax.set_title('Matriz de Correlaci√≥n')
st.pyplot(fig)

############################################################################################



# Verificar los nombres de las columnas
st.write("### Columnas en el archivo de datos:")
st.write(data.columns)

# Preprocesamiento de datos
def preprocess_data(data):
    # Aseg√∫rate de que los nombres de las columnas coincidan exactamente con los del archivo
    X = data.drop(columns=['IID', 'Riesgo_Cardiovascular', 'Riesgo_Cardiovascular_Binario'])
    y = data['Riesgo_Cardiovascular_Binario']
    X = pd.get_dummies(X, columns=['Sexo'], drop_first=True)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X_scaled, y

try:
    X_scaled, y = preprocess_data(data)
except KeyError as e:
    st.error(f"Error: {e}. Verifica los nombres de las columnas en el archivo de datos.")
    st.stop()

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Funci√≥n para graficar PCA y t-SNE
def plot_dimension_reduction(X_pca, X_tsne, y, title_pca, title_tsne):
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='coolwarm', ax=ax[0])
    ax[0].set_title(title_pca)
    
    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='coolwarm', ax=ax[1])
    ax[1].set_title(title_tsne)
    
    st.pyplot(fig)

# Funci√≥n para graficar curva ROC
def plot_roc_curve(y_true, y_pred_proba, title):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc='lower right')
    st.pyplot(plt)

# Aplicar PCA y t-SNE
st.write("### Reducci√≥n de Dimensionalidad")
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

plot_dimension_reduction(X_pca, X_tsne, y, 'PCA - Datos Originales', 't-SNE - Datos Originales')

# Entrenar y evaluar modelos con SVM
st.write("## Modelos de Machine Learning")

# SVM con datos originales
st.write("### SVM con Datos Originales")
svm_model = SVC(kernel='linear', probability=True, random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)
y_pred_proba_svm = svm_model.predict_proba(X_test)[:, 1]
accuracy_svm = accuracy_score(y_test, y_pred_svm)
st.write(f"**Precisi√≥n (SVM):** {accuracy_svm:.2f}")
st.write("**Matriz de Confusi√≥n (SVM):**")
st.write(confusion_matrix(y_test, y_pred_svm))
st.write("**Informe de Clasificaci√≥n (SVM):**")
st.write(classification_report(y_test, y_pred_svm))
plot_roc_curve(y_test, y_pred_proba_svm, 'Curva ROC - SVM (Datos Originales)')

st.write("""
**Conclusi√≥n:**
El modelo SVM con datos originales logra una precisi√≥n del {:.2f}. La matriz de confusi√≥n muestra un buen equilibrio entre verdaderos positivos y falsos positivos. La curva ROC con un AUC de {:.2f} indica un buen rendimiento en la clasificaci√≥n.
""".format(accuracy_svm, auc(roc_curve(y_test, y_pred_proba_svm)[0], roc_curve(y_test, y_pred_proba_svm)[1])))

# Reducci√≥n de dimensionalidad con PCA y t-SNE para entrenamiento de modelos
X_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y, test_size=0.3, random_state=42)
X_train_tsne, X_test_tsne, _, _ = train_test_split(X_tsne, y, test_size=0.3, random_state=42)

# Entrenar y evaluar modelos con SVM usando PCA y t-SNE
for name, X_tr, X_te in [('PCA', X_train_pca, X_test_pca), ('t-SNE', X_train_tsne, X_test_tsne)]:
    st.write(f"### SVM con {name}")
    model = SVC(kernel='linear', probability=True, random_state=42)
    model.fit(X_tr, y_train)
    y_pred = model.predict(X_te)
    y_pred_proba = model.predict_proba(X_te)[:, 1]
    accuracy = accuracy_score(y_test, y_pred)
    st.write(f"**Precisi√≥n (SVM con {name}):** {accuracy:.2f}")
    st.write(f"**Matriz de Confusi√≥n (SVM con {name}):**")
    st.write(confusion_matrix(y_test, y_pred))
    st.write(f"**Informe de Clasificaci√≥n (SVM con {name}):**")
    st.write(classification_report(y_test, y_pred))
    plot_roc_curve(y_test, y_pred_proba, f'Curva ROC - SVM ({name})')

    st.write(f"""
    **Conclusi√≥n:**
    El modelo SVM con {name} logra una precisi√≥n del {accuracy:.2f}. La reducci√≥n de dimensionalidad con {name} permite visualizar mejor la estructura de los datos, pero puede afectar ligeramente el rendimiento del modelo. La curva ROC con un AUC de {auc(roc_curve(y_test, y_pred_proba)[0], roc_curve(y_test, y_pred_proba)[1]):.2f} sigue siendo competitiva.
    """)

# Definir hiperpar√°metros de la red neuronal
hyperparams = {'depth': 4, 'epochs': 43, 'num_units': 144, 'optimizer': 'sgd', 'activation': 'relu', 'batch_size': 72, 'learning_rate': 0.0329}

# Funci√≥n para crear la red neuronal
def create_nn_model(input_dim, hyperparams):
    model = Sequential()
    for _ in range(hyperparams['depth'] - 1):
        model.add(Dense(hyperparams['num_units'], activation=hyperparams['activation']))
    model.add(Dense(1, activation='sigmoid'))
    optimizer = SGD(learning_rate=hyperparams['learning_rate'])
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Entrenar y evaluar red neuronal con datos originales
st.write("### Red Neuronal con Datos Originales")
nn_model = create_nn_model(X_train.shape[1], hyperparams)
history = nn_model.fit(X_train, y_train, epochs=hyperparams['epochs'], batch_size=hyperparams['batch_size'], validation_split=0.2, verbose=0)
y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int)
y_pred_proba_nn = nn_model.predict(X_test).flatten()
accuracy_nn = accuracy_score(y_test, y_pred_nn)
st.write(f"**Precisi√≥n (Red Neuronal):** {accuracy_nn:.2f}")
st.write("**Matriz de Confusi√≥n (Red Neuronal):**")
st.write(confusion_matrix(y_test, y_pred_nn))
st.write("**Informe de Clasificaci√≥n (Red Neuronal):**")
st.write(classification_report(y_test, y_pred_nn))
plot_roc_curve(y_test, y_pred_proba_nn, 'Curva ROC - Red Neuronal')

# Gr√°fica de precisi√≥n y p√©rdida durante el entrenamiento
fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].plot(history.history['accuracy'], label='Precisi√≥n en entrenamiento')
ax[0].plot(history.history['val_accuracy'], label='Precisi√≥n en validaci√≥n')
ax[0].set_title('Precisi√≥n durante el Entrenamiento')
ax[0].set_xlabel('√âpocas')
ax[0].set_ylabel('Precisi√≥n')
ax[0].legend()

ax[1].plot(history.history['loss'], label='P√©rdida en entrenamiento')
ax[1].plot(history.history['val_loss'], label='P√©rdida en validaci√≥n')
ax[1].set_title('P√©rdida durante el Entrenamiento')
ax[1].set_xlabel('√âpocas')
ax[1].set_ylabel('P√©rdida')
ax[1].legend()
st.pyplot(fig)

st.write("""
**Conclusi√≥n:**
La red neuronal con datos originales logra una precisi√≥n del {:.2f}. La curva de aprendizaje muestra que el modelo converge adecuadamente, sin signos de sobreajuste. La curva ROC con un AUC de {:.2f} confirma un buen rendimiento en la clasificaci√≥n.
""".format(accuracy_nn, auc(roc_curve(y_test, y_pred_proba_nn)[0], roc_curve(y_test, y_pred_proba_nn)[1])))

# Evaluar red neuronal con PCA y t-SNE
for name, X_tr, X_te in [('PCA', X_train_pca, X_test_pca), ('t-SNE', X_train_tsne, X_test_tsne)]:
    st.write(f"### Red Neuronal con {name}")
    nn_model = create_nn_model(X_tr.shape[1], hyperparams)
    history = nn_model.fit(X_tr, y_train, epochs=hyperparams['epochs'], batch_size=hyperparams['batch_size'], validation_split=0.2, verbose=0)
    y_pred_nn = (nn_model.predict(X_te) > 0.5).astype(int)
    y_pred_proba_nn = nn_model.predict(X_te).flatten()
    accuracy_nn = accuracy_score(y_test, y_pred_nn)
    st.write(f"**Precisi√≥n (Red Neuronal con {name}):** {accuracy_nn:.2f}")
    st.write(f"**Matriz de Confusi√≥n (Red Neuronal con {name}):**")
    st.write(confusion_matrix(y_test, y_pred_nn))
    st.write(f"**Informe de Clasificaci√≥n (Red Neuronal con {name}):**")
    st.write(classification_report(y_test, y_pred_nn))
    plot_roc_curve(y_test, y_pred_proba_nn, f'Curva ROC - Red Neuronal ({name})')

    # Gr√°ficas de precisi√≥n y p√©rdida durante el entrenamiento
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))
    ax[0].plot(history.history['accuracy'], label='Precisi√≥n en entrenamiento')
    ax[0].plot(history.history['val_accuracy'], label='Precisi√≥n en validaci√≥n')
    ax[0].set_title(f'Precisi√≥n durante el Entrenamiento ({name})')
    ax[0].set_xlabel('√âpocas')
    ax[0].set_ylabel('Precisi√≥n')
    ax[0].legend()

    ax[1].plot(history.history['loss'], label='P√©rdida en entrenamiento')
    ax[1].plot(history.history['val_loss'], label='P√©rdida en validaci√≥n')
    ax[1].set_title(f'P√©rdida durante el Entrenamiento ({name})')
    ax[1].set_xlabel('√âpocas')
    ax[1].set_ylabel('P√©rdida')
    ax[1].legend()
    st.pyplot(fig)

    st.write(f"""
    **Conclusi√≥n:**
    La red neuronal con {name} logra una precisi√≥n del {accuracy_nn:.2f}. La curva de aprendizaje muestra que el modelo converge adecuadamente, sin signos de sobreajuste. La curva ROC con un AUC de {auc(roc_curve(y_test, y_pred_proba_nn)[0], roc_curve(y_test, y_pred_proba_nn)[1]):.2f} confirma un buen rendimiento en la clasificaci√≥n.
    """)

####################################################

st.title(" Predicci√≥n de Riesgo Cardiovascular")

# Cargar modelo desde archivo comprimido
@st.cache_resource
def load_model():
    """Carga el modelo y el scaler desde un archivo comprimido."""
    try:
        with gzip.open("best_model.pkl.gz", "rb") as f:
            data = pickle.load(f)  # Carga el diccionario con el modelo y scaler

        if isinstance(data, dict):
            model = data.get("modelo", None)  # Extraer modelo
            scaler = data.get("scaler", None)  # Extraer scaler
            if model is None or scaler is None:
                raise ValueError("El archivo no contiene el modelo o el scaler.")
            return model, scaler
        else:
            raise ValueError("El archivo no tiene el formato esperado.")
    except Exception as e:
        st.error(f"Error al cargar el modelo: {e}")
        return None, None

# Cargar el modelo y el scaler
model, scaler = load_model()

# Funci√≥n para ingresar datos del usuario con valores predeterminados
def user_input():
    sexo = st.selectbox("Sexo", ["Femenino", "Masculino"], index=1)  # Predeterminado: Masculino
    edad = st.number_input("Edad", min_value=18, max_value=100, step=1, value=45)
    leptina = st.number_input("Leptina", min_value=0.0, step=0.1, value=15.2)
    grasa = st.number_input("Grasa (%)", min_value=0.0, step=0.1, value=22.5)
    imc = st.number_input("IMC", min_value=10.0, step=0.1, value=27.3)
    bai = st.number_input("BAI", min_value=0.0, step=0.1, value=25.1)
    cintura = st.number_input("Cintura (cm)", min_value=30.0, step=0.1, value=92.4)
    cadera = st.number_input("Cadera (cm)", min_value=30.0, step=0.1, value=100.2)
    cvldl = st.number_input("CVLDL", min_value=0.0, step=0.1, value=24.0)
    triglic = st.number_input("Triglic√©ridos", min_value=0.0, step=0.1, value=130.5)
    ctotal = st.number_input("Colesterol Total", min_value=0.0, step=0.1, value=210.8)
    cldl = st.number_input("CLDL", min_value=0.0, step=0.1, value=140.3)
    chdl = st.number_input("CHDL", min_value=0.0, step=0.1, value=48.2)
    fto_aditivo = st.number_input("FTO Aditivo", min_value=0.0, step=0.1, value=1.5)
    
    sexo_binario = 1 if sexo == "Masculino" else 0  # Codificar Sexo
    
    data = np.array([[sexo_binario, edad, leptina, grasa, imc, bai, cintura, cadera, 
                      cvldl, triglic, ctotal, cldl, chdl, fto_aditivo]], dtype=np.float32)

    return data

# Obtener datos del usuario
input_data = user_input()

# Bot√≥n para hacer la predicci√≥n
if st.button(" Realizar Predicci√≥n"):
    if model is not None and scaler is not None:
        try:
            # Escalar los datos con el scaler cargado desde el modelo
            input_data_scaled = scaler.transform(input_data)

            # Realizar la predicci√≥n
            prediction = model.predict(input_data_scaled)

            prediction_label = "üî¥ Alto Riesgo" if prediction[0] >= 0.5 else "üü¢ Bajo Riesgo"
            st.subheader("Resultado de la Predicci√≥n:")
            st.markdown(f"## {prediction_label}")

        except Exception as e:
            st.error(f"Error en la predicci√≥n: {e}")
    else:
        st.error("No se pudo cargar el modelo y/o el scaler.")
